{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-12T03:52:11.444008Z",
     "start_time": "2025-03-12T03:52:11.399950Z"
    }
   },
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "rs = 132\n",
    "dataset = pd.read_csv('./data/dataset.csv', dtype={\"股票代码\": \"object\"})\n",
    "dataset.set_index(['股票代码', \"截止日期\"], inplace=True)\n",
    "# 1=国企、2=民营、3=外资、4=其他\n",
    "features = ['净资产收益率(%)', '资产报酬率(%)', '营业收入增长率(%)', '成本费用利润率(%)', '总资产周转率(%)', '应收账款周转率(%)', '存货周转率(%)', '营业周期', '技术人员占比(%)', '研发营收比(%)', '发明专利申请数', '数字化软硬件投入比(%)', '数字化战略导向前瞻性', '数字化战略导向持续性', '数字化战略导向广度', '数字化战略导向强度', '数字发明专利', '数字国家级奖项', '数字创新论文', '数字创新标准', '数字创新资质', '管理层数字职务设立', '两权分离率(%)', '数字人力计划投入', '科技创新基地建设', '员工人均营收比(%)', '提供岗位增长率(%)', '员工收入增长率(%)', '社会责任报告质量', '供应链合作伙伴', '数字化供应链覆盖度', '客户集中度(%)', '供应商集中度(%)']\n",
    "label_name = 'score'\n",
    "# 获取数据集和标签值\n",
    "y : pd.Series = dataset[label_name]\n",
    "X : pd.DataFrame = dataset[features].copy(deep=True).astype(\"float\")\n",
    "# 数据预处理：1.极差标准化；2.数据集划分。\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=rs)\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_s = scaler.transform(X_train)\n",
    "X_test_s = scaler.transform(X_test)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T03:52:14.564548Z",
     "start_time": "2025-03-12T03:52:14.299004Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.model_selection import KFold, GridSearchCV, cross_val_score\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "# 一般机器学习模型的训练与优化\n",
    "\n",
    "# 1.最小二乘线性回归\n",
    "model = LinearRegression(n_jobs=-1)\n",
    "model.fit(X_train_s, y_train)\n",
    "y_pred = model.predict(X_test_s)\n",
    "print(f\"OLS Regression | r2:{r2_score(y_test, y_pred)} | mean_squared_error:{mean_squared_error(y_test, y_pred):.2} | mean_absolute_error:{mean_absolute_error(y_test, y_pred):.2}。\\n 因子分析的结果实际是对原结果进行线性转化的过程，因此最小二乘法能够较为精确的计算出实际权重。\")"
   ],
   "id": "131fc53b9ebe80e3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS Regression | r2:0.9962521498539745 | mean_squared_error:2.6e-06 | mean_absolute_error:0.00055。\n",
      " 因子分析的结果实际是对原结果进行线性转化的过程，因此最小二乘法能够较为精确的计算出实际权重。\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# 如果回归不好就做分类，分5类\n",
    "\n",
    "model = DecisionTreeRegressor(random_state=rs)\n",
    "path = model.cost_complexity_pruning_path(X_train_s, y_train)\n",
    "param_grid = {'ccp_alpha': path.ccp_alphas}\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "model = GridSearchCV(DecisionTreeRegressor(random_state=rs), param_grid, cv=kfold,n_jobs=-1)\n",
    "model.fit(X_train_s, y_train)\n",
    "print(model.best_params_)"
   ],
   "id": "c8cf7dc0fcb1584c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# 4.catboost参数优化\n",
    "params = {\n",
    "    \"objective\": 'mse',\n",
    "    'max_depth': 2,\n",
    "    \"n_estimators\": 100,\n",
    "    # \"learning_rate\": 0.1,\n",
    "    'min_child_samples': 5,\n",
    "    'reg_alpha': 0,\n",
    "    'reg_lambda': 0,\n",
    "    \"force_col_wise\": True,\n",
    "    \"subsample\": 0.8,\n",
    "    'colsample_bytree': 0.32,\n",
    "    \"num_leaves\": 7  # 一般设置为(0, 2^max_depth - 1]的一个数值。是一个需要重点调节的参数，对模型性能影响很大。\n",
    "}\n",
    "# \n",
    "param_grid = {\n",
    "    'max_depth': [2, 3, 6],\n",
    "    'learning_rate': np.arange(0.29, 0.35,0.01),  # [0.06, 0.1, 0.12, 0.17, 0.23, 0.24, 0.25, 0.27]\n",
    "    \"n_estimators\": 100,\n",
    "    'min_child_samples': [2,3, 5], # 6附近 \n",
    "    'colsample_bytree': np.arange(0.3, 0.4, 0.01),  # 0.25和0.45附近\n",
    "    \"num_leaves\": [6,7,8]\n",
    "}\n",
    "\n",
    "# iterations=400,, border_count=182\n",
    "total_params  = param_grid['learning_rate']\n",
    "scores = {}\n",
    "for i in range(len(total_params)):\n",
    "    # ccp_alpha=params[i],\n",
    "    model = LGBMRegressor(learning_rate=total_params[i],n_jobs=-1, random_state=rs, verbosity=-1, **params)\n",
    "    model.fit(X_train_s, y_train)\n",
    "    # score = cross_val_score(model, X_test_s, y_test, cv=5).mean()\n",
    "    score= model.score(X_test_s, y_test)\n",
    "    scores[total_params[i]] = score\n",
    "# print(scores)\n",
    "plt.figure(figsize=(20,10))\n",
    "pd.Series(scores).plot()\n",
    "plt.show()\n"
   ],
   "id": "6591e3691387c6e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# 基于树模型的参数优化基本流程为：首先计算单个参数的大致范围，随后进行网格搜索。\n",
    "# 决策树算法优化思路:\n",
    "\n",
    "# 1.优化ccp_alpha(网格搜索)；\n",
    "\n",
    "# model = DecisionTreeRegressor(random_state=rs)\n",
    "# path = model.cost_complexity_pruning_path(X_train_s, y_train)\n",
    "# param_grid = {'ccp_alpha': path.ccp_alphas}\n",
    "# kfold = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "# model = GridSearchCV(DecisionTreeRegressor(random_state=rs), param_grid, cv=kfold)\n",
    "# model.fit(X_train_s, y_train)\n",
    "# print(model.best_params_)\n",
    "\n",
    "model = DecisionTreeRegressor(max_features=\"sqrt\", random_state=1)\n",
    "path = model.cost_complexity_pruning_path(X_train_s, y_train)\n",
    "param_grid = {\n",
    "    'ccp_alpha': path.ccp_alphas,  # 剪枝参数\n",
    "    'max_depth': np.arange(4, 12, 1),  # 决策树最大深度，用来防止过拟合; \n",
    "    'min_samples_split': np.arange(6, 9, 1),  # 分裂节点所需的最小样本数，也就是如果样本数小于这个值就不划分了。\n",
    "    'min_samples_leaf':[1,2,3,5,6,7]  # 叶节点所需的最小样本数，如果样本数小于这个，就不划分了。用来防止过拟合\n",
    "}\n",
    "# 2.网格法参数微调\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=rs)\n",
    "model = GridSearchCV(DecisionTreeRegressor(max_features=\"sqrt\", random_state=rs), param_grid, cv=kfold, n_jobs=-1)\n",
    "model.fit(X_train_s, y_train)\n",
    "\n",
    "# 决策树最优超参数：{'ccp_alpha': 0.7814179316082759, 'max_depth': 11, 'min_samples_leaf': 1, 'min_samples_split': 2} ；综合得分0.7849455038096266\n",
    "print(f\"决策树最优超参数：{model.best_params_}\\n 综合得分{model.best_estimator_.score(X_test_s, y_test)}\")"
   ],
   "id": "5ee823de33dbfc86",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# 随机森林参数优化：https://www.jianshu.com/p/f5b45a60289f\n",
    "param_grid = {\n",
    "    'max_depth': np.arange(10, 16, 1),  # 树的最大深度\n",
    "    'n_estimators': [3,4,5,6,7,8,9, 120,121,122,123,124,125,126,127,128,129],  # 森林中树的数量\n",
    "    'min_samples_split': [2, 3, 4, 5, 12, 13,14,15],  # 分裂节点所需的最小样本数\n",
    "    'min_samples_leaf':[1, 2, 3],  # 叶节点所需的最小样本数\n",
    "    'max_features': np.arange(8, 30, 1)  # 每次分裂时考虑的特征数量\n",
    "}\n",
    "\n",
    "model = RandomForestRegressor(verbose=0, random_state=rs, min_samples_split=2, min_samples_leaf=2)\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=rs)\n",
    "model = GridSearchCV(model, param_grid, cv=kfold, scoring='r2', verbose=0, n_jobs=-1)\n",
    "model.fit(X_train_s, y_train)\n",
    "# RandomForest 最优超参数：{'max_depth': 13, 'max_features': 11, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 7}\n",
    " # 综合得分0.8483375813154402\n",
    "\n",
    "print(f\"RandomForest 最优超参数：{model.best_params_}\\n 综合得分{model.best_estimator_.score(X_test_s, y_test)}\")"
   ],
   "id": "d27429b14924eb44",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "# 3.xgboost参数优化：https://www.cnblogs.com/showmeai/p/16037327.html；\n",
    "params = {\n",
    "   \"objective\": 'reg:squarederror',\n",
    "   \"n_estimators\": 130,\n",
    "   \"max_depth\": 2,\n",
    "   \"learning_rate\": 0.3,\n",
    "   \"subsample\": 0.8,\n",
    "   \"min_child_weight\": 1,\n",
    "   \"gamma\": 0,\n",
    "   \"colsample_bytree\": 0.8,\n",
    "   \"reg_alpha\": 0,\n",
    "   \"reg_lambda\": 0,\n",
    "   \"scale_pos_weight\": 1,\n",
    "}\n",
    "param_grid = {\n",
    "    'n_estimators':np.arange(25,40,1),\n",
    "    'max_depth': [2, 3, 4],\n",
    "    'learning_rate': np.arange(0.26, 0.32, 0.01), #\n",
    "    # \"min_child_weight\": np.arange(1,2, 1),\n",
    "    \"gamma\": [0, 0.1],\n",
    "    # 'subsample': np.arange(0.2, 1, 0.1)\n",
    "}\n",
    "\n",
    "model = XGBRegressor(random_state=rs)\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=rs)\n",
    "model = GridSearchCV(model, param_grid, cv=kfold, scoring='r2', verbose=0, n_jobs=-1)\n",
    "model.fit(X_train_s, y_train)  \n",
    "print(f\"XGBRegressor 最优超参数：{model.best_params_}\\n 综合得分{model.best_estimator_.score(X_test_s, y_test)}\")\n",
    "\n",
    "# XGBRegressor 最优超参数：{'gamma': 0, 'learning_rate': 0.30000000000000004, 'max_depth': 3, 'n_estimators': 39}\n",
    "#  综合得分0.9193579436564832"
   ],
   "id": "5f9d54a550d7bc21",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# 4.catboost参数优化\n",
    "param_grid = {\n",
    "    # 'iterations': np.arange(300, 401, 1),\n",
    "    'depth': [2, 3, 4],\n",
    "    'learning_rate': [0.34,0.35,0.36,0.37], \n",
    "    'l2_leaf_reg': np.arange(0.20, 0.25, 0.01),\n",
    "    # 'bagging_temperature': np.arange(0.1, 1, 0.1),\n",
    "    'border_count': np.arange(48, 54, 1)\n",
    "}\n",
    "# 分布计算, border_count=182\n",
    "model = CatBoostRegressor(random_state=rs, verbose=0, train_dir=None, allow_writing_files=False, iterations=400, bagging_temperature=0.1)\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=rs)\n",
    "model = GridSearchCV(model, param_grid, cv=kfold, scoring='r2', verbose=0, n_jobs=-1)\n",
    "model.fit(X_train_s, y_train) \n",
    "print(f\"CatBoost 最优超参数：{model.best_params_}\\n 综合得分{model.best_estimator_.score(X_test_s, y_test)}\")\n",
    "\n",
    "# CatBoost 最优超参数：{'border_count': 49, 'depth': 2, 'l2_leaf_reg': 0.21000000000000002, 'learning_rate': 0.36}\n",
    "#  综合得分0.9480621547116888"
   ],
   "id": "6cb6ff27c8f16f51",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# 4.catboost参数优化\n",
    "params = {\n",
    "    \"objective\": 'mse',\n",
    "    'max_depth': 2,\n",
    "    \"n_estimators\": 100,\n",
    "    \"learning_rate\": 0.29,\n",
    "    'min_child_samples': 5,\n",
    "    'reg_alpha': 0,\n",
    "    'reg_lambda': 0,\n",
    "    \"force_col_wise\": True,\n",
    "    \"subsample\": 0.8,\n",
    "    'colsample_bytree': 0.32,\n",
    "    \"num_leaves\": 7  # 一般设置为(0, 2^max_depth - 1]的一个数值。是一个需要重点调节的参数，对模型性能影响很大。\n",
    "}\n",
    "# \n",
    "param_grid = {\n",
    "    'max_depth': [2, 3, 6],\n",
    "    'learning_rate': np.arange(0.29, 0.35,0.01),  # [0.06, 0.1, 0.12, 0.17, 0.23, 0.24, 0.25, 0.27]\n",
    "    # \"n_estimators\": 100,\n",
    "    'min_child_samples': [2,3, 5], # 6附近 \n",
    "    'colsample_bytree': np.arange(0.3, 0.4, 0.01),  # 0.25和0.45附近\n",
    "    \"num_leaves\": [6,7,8]\n",
    "}\n",
    "model = LGBMRegressor(n_jobs=-1, random_state=rs, verbosity=-1, **params)\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=rs)\n",
    "model = GridSearchCV(model, param_grid, cv=kfold, scoring='r2', verbose=0, n_jobs=-1)\n",
    "model.fit(X_train_s, y_train) \n",
    "print(f\"LGBMBoost 最优超参数：{model.best_params_}\\n 综合得分{model.best_estimator_.score(X_test_s, y_test)}\")"
   ],
   "id": "7fb2d0875d95af1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "params = {\n",
    "    \"objective\": 'mse',\n",
    "    'max_depth': 3,\n",
    "    \"n_estimators\": 200,\n",
    "    'learning_rate': 0.15,\n",
    "    'min_child_samples': 7, \n",
    "    'reg_alpha': 0,\n",
    "    'reg_lambda': 0,\n",
    "    \"force_col_wise\": True,\n",
    "    \"subsample\": 0.8,\n",
    "    'colsample_bytree': 0.32,\n",
    "    \"num_leaves\": 6\n",
    "}\n",
    "model = LGBMRegressor(n_jobs=-1, random_state=rs, verbosity=-1, **params)\n",
    "model.fit(X_train_s, y_train)\n",
    "print(model.score(X_test_s, y_test))"
   ],
   "id": "671edd0789e1e7d1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from tensorflow import keras\n",
    "from utils.methods import r2\n",
    "\n",
    "# Sequential 模型适用于普通层堆栈 其中，每层只有一个 input Tensor 和一个 Output Tensor。\n",
    "model = keras.models.Sequential()\n",
    "# model.add(keras.Input(shape=(X_train_s.shape[1], )))\n",
    "model.add(keras.layers.Dense(units=15, activation='relu', name=\"layer1\", input_shape=(X_train_s.shape[1], ),\n",
    "                               kernel_regularizer=keras.regularizers.l2(0.02)))\n",
    "model.add(keras.layers.Dropout(0.02))\n",
    "\n",
    "# 增加输出层\n",
    "model.add(keras.layers.Dense(units=1, name=\"output\"))\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
    "model.compile(optimizer=optimizer, loss='mse', metrics=['mse', r2, 'mae'])\n",
    "# 查看模型结构\n",
    "# utils.plot_model(model, \"./assert/feature_importance/bp_model_structure.png\", show_shapes=True)\n",
    "# model.summary()\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)\n",
    "hist = model.fit(X_train_s, y_train, validation_split=0.3, epochs=80, batch_size=150, shuffle=False, verbose=0,\n",
    "                         callbacks=[early_stopping])\n",
    "# validation_split是训练集验证集拆分，epochs代表训练300轮，batch_size代表在批量梯度下降时每次选择16个样本，shuffle代表在训练过程中不会将数据反复打乱\n",
    "# verbose：日志显示，0为不在标准输出流输出日志信息，1为输出进度条记录，2为每个epoch输出一行记录。\n",
    "\n",
    "s = model.evaluate(X_test_s, y_test, verbose=0)\n",
    "print(s)\n",
    "fig, ax = plt.subplots(figsize=(8, 6), dpi=100)\n",
    "# 绘制训练集验证集损失变化\n",
    "\"\"\"\n",
    "训练过程中的损失函数变化情况\n",
    "模型在训练集上的损失远小于验证集的损失，说明模型过度拟合，但总体准确度已经较为准确。\n",
    "在训练的后期（大约150个epoch之后），验证集MSE下降速度很慢。这可能表明模型容量过大或正则化不足，可以适当增加正则化系数。\n",
    "\"\"\"\n",
    "ax.plot(hist.history['mse'], 'k', label='Train')\n",
    "ax.plot(hist.history['val_mse'], 'b', label='Validation')\n",
    "# plt.axvline(index,linestyle='--', color='k')\n",
    "ax.set_ylabel('MSE')\n",
    "ax.set_xlabel('Epoch')\n",
    "# ax.title('Mean Squared Error')\n",
    "# model.save(\"./assert/temp/bp_keras_model.keras\")\n",
    "# pd.DataFrame(hist.history).to_csv('./assert/bp_model_loss.csv')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "f2ecf0de782ec114",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "params = {\n",
    "    \"objective\": 'mse',\n",
    "    'max_depth': 4,\n",
    "    \"n_estimators\": 100,\n",
    "    \"learning_rate\": 0.1,\n",
    "    'min_child_samples': 6,\n",
    "    'reg_alpha': 0,\n",
    "    'reg_lambda': 0,\n",
    "    \"force_col_wise\": True,\n",
    "    \"subsample\": 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    # \"num_leaves\": 7  # 一般设置为(0, 2^max_depth - 1]的一个数值。是一个需要重点调节的参数，对模型性能影响很大。\n",
    "}\n",
    "param_range = np.arange(2, 7, 1)\n",
    "scores = {}\n",
    "for i in param_range:\n",
    "    model = LGBMRegressor(n_jobs=-1, random_state=rs, verbosity=-1, num_leaves=i, **params)\n",
    "    model.fit(X_train_s, y_train)\n",
    "    scores[i] = model.score(X_test_s, y_test)\n",
    "\n",
    "temp_data = pd.Series(scores)\n",
    "# _ind = min(scores, key=scores.get)\n",
    "plt.figure(figsize=[20, 5])\n",
    "temp_data.plot()\n",
    "plt.show()"
   ],
   "id": "d206e2e2198ec58a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
