{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T19:03:05.146805Z",
     "start_time": "2024-12-11T19:03:05.040521Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "rs = 132\n",
    "np.random.seed(1)\n",
    "dataset = pd.read_csv('./data/dataset.csv')\n",
    "# '营业利润增长率'/'应收账款周转率'\n",
    "features = [\n",
    "    '净资产收益率', '投入资本回报率', '成本费用利润率',\n",
    "    '总资产周转率', '流动资产周转率', '存货周转率', '资产负债率',\n",
    "    '速动比率', '现金流动负债比率', '营业总收入增长率', '总资产增长率',\n",
    "    '员工收入增长率(%)', '发明专利', '研发人员占比(%)', '研发营收比(%)',\n",
    "    '股权集中度(%)', '两权分离率(%)'\n",
    "]\n",
    "label_name = '因子得分'\n",
    "# 建立企业id与名称的索引\n",
    "index_map = dataset[\"股票简称\"].to_dict()\n",
    "# 获取数据集和标签值\n",
    "y_test : pd.Series = dataset[label_name]\n",
    "X : pd.DataFrame = dataset[features].copy(deep=True).astype(\"float\")\n",
    "# 数据预处理：1.极差标准化；2.数据集划分。\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_test, test_size=0.2, random_state=rs)\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "# scaler.fit(X)\n",
    "\n",
    "# X_train_s = scaler.transform(X)\n",
    "X_train_s = scaler.transform(X_train)\n",
    "X_test_s = scaler.transform(X_test)"
   ],
   "id": "8ad04400cb122b8a",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T19:03:26.019614Z",
     "start_time": "2024-12-11T19:03:13.053095Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from config import r2_score\n",
    "from tensorflow.keras import models, utils, layers, regularizers, callbacks, optimizers\n",
    "\n",
    "\n",
    "# Sequential 模型适用于普通层堆栈 其中，每层只有一个 input Tensor 和一个 Output Tensor。\n",
    "model = models.Sequential()\n",
    "# model.add(keras.Input(shape=(X_train_s.shape[1], )))\n",
    "model.add(layers.Dense(units=15, activation='relu', name=\"layer1\", input_shape=(X_train_s.shape[1], ),\n",
    "                               kernel_regularizer=regularizers.l2(0.02)))\n",
    "model.add(layers.Dropout(0.08))\n",
    "# 增加输出层\n",
    "model.add(layers.Dense(units=1, name=\"output\"))\n",
    "optimizer = optimizers.Adam(learning_rate=9e-3)\n",
    "model.compile(optimizer=optimizer, loss='mse', metrics=['mse', r2_score, 'mae'])\n",
    "# 查看模型结构\n",
    "# utils.plot_model(model, \"./assert/feature_importance/bp_model_structure.png\", show_shapes=True)\n",
    "# model.summary()\n",
    "early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "hist = model.fit(X_train_s, y_train, validation_split=0.3, epochs=300, batch_size=30, shuffle=False, verbose=0,\n",
    "                         callbacks=[early_stopping])\n",
    "# validation_split是训练集验证集拆分，epochs代表训练300轮，batch_size代表在批量梯度下降时每次选择16个样本，shuffle代表在训练过程中不会将数据反复打乱\n",
    "# verbose：日志显示，0为不在标准输出流输出日志信息，1为输出进度条记录，2为每个epoch输出一行记录。\n",
    "s = model.evaluate(X_test_s, y_test, verbose=0)\n",
    "print(f'测试集mse得分:{s[1]:.3%}，R^2得分{s[2]:.3%}')\n",
    "\"\"\"\n",
    "#### 关于损失函数\n",
    "MSE（Mean Squared Error）均方误差是回归问题中常用的损失函数之一。MSE损失函数可以用于量化预测值与真实值之间的差距，并且对误差进行平方处理，使得较大的误差对损失函数的影响更加显著。其优势在于由于对结果进行了平方处理，使得对异常值表现比较敏感，因此可以比较准确的反映出模型预测的准确性，很适合用于回归问题。</br>缺点在于，对异常值过于敏感，易因异常值原因导致模型偏差较大，在计算损失时，选择了等权重方法，现实中样本的重要性可能不同。\n",
    "其定义为：\n",
    "$$ MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $$\n",
    "其中，$y_i$为真实值，$\\hat{y}_i$为模型预测值，$n$为样本数量。\n",
    "**其他损失函数**\n",
    "- 加权MSE：适用于针对样本差异性大的情形，加权MSE可以减少异常值的影响，提高模型的鲁棒性。$np.mean(weights*(y_{true}-y_{pred})**2)$\n",
    "- MAE：取绝对值而不是取平方，优点是完美表达损失，不易受异常值（偏差）影响，稳健型强。缺点在于不可导，无法使用梯度下降算法，故优化速度慢。\n",
    "- Huber：中和了MSE和MAE，具有一定的抗异常值的特性，但同时中和了其缺点，包含大量的额外参数。\n",
    "- Log-cosh: 在损失函数为较大值时收敛速度较快，对异常值有一定容忍度。适用于误差比较大的数据集，其计算相对复杂。\n",
    "</br>\n",
    "</br>\n",
    "#### 可用的优化器\n",
    "SGD（随机梯度下降）：最基本的优化器，每次只使用一个样本来更新权重。</br>\n",
    "Momentum：结合了动量概念，加速SGD在相关方向上的收敛，同时抑制震荡。</br>\n",
    "Adam：自适应学习率的优化器，结合了Momentum和RMSProp的特点。</br>\n",
    "Adagrad：对每个参数使用不同的学习率，但随着迭代次数的增加，学习率可能会变得非常大。</br>\n",
    "Adadelta：改进版的Adagrad，使用动态学习率。</br>\n",
    "RMSProp：与Adadelta类似，但使用指数移动平均来计算方差。</br>\n",
    "FTRL：适用于在线学习问题的优化器。</br>\n",
    "Proximal Gradient Descent：用于求解约束优化问题。</br>\n",
    "FTRL-Proximal Gradient Descent：结合了FTRL和Proximal Gradient Descent。</br>\n",
    "其中，SGD和Adam适用于简单的回归和分类问题，Adam在复杂的任务或大规模数据集表现较好，Adagrad、Adadelta或RMSProp能够更精细的控制学习率，FTRL和Proximal Gradient Descent则适用于在线学习或分布式系统，如果问题包含特定的约束条件或问题结构，如稀疏性或低维性等，则需要选择特定的优化器，如FTRL或Proximal Gradient Descent。\n",
    "\"\"\"\n",
    "model.save(\"./assert/bp_keras_model.keras\")\n",
    "pd.DataFrame(hist.history).to_csv('./assert/bp_model_loss.csv')"
   ],
   "id": "6eecbf37a0480409",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-12 03:03:13.244591: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-12 03:03:13.254921: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1733943793.266507   10013 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1733943793.270203   10013 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-12 03:03:13.284419: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "I0000 00:00:1733943795.281657   10013 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9558 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1733943796.450247   10141 service.cc:148] XLA service 0x7ff460015550 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1733943796.450618   10141 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 4070, Compute Capability 8.9\n",
      "2024-12-12 03:03:49.294505: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1733943829.352430   10141 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "I0000 00:00:1733943796.949118   10141 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试集mse得分:0.311%，R^2得分97.325%\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T19:05:04.657987Z",
     "start_time": "2024-12-11T19:04:31.180826Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from config import table_translate\n",
    "\n",
    "result = pd.DataFrame(data=[\"ANN\", s[2], s[1], s[3]], index=[\"model_name\", \"r2_score\", \"mse\", \"mae\"]).T\n",
    "model = LinearRegression(n_jobs=-1)\n",
    "model.fit(X_train_s, y_train)\n",
    "y_pred = model.predict(X_test_s)\n",
    "result.loc[result.shape[0]] = [\"OLS Regression\", r2_score(y_test, y_pred), mean_squared_error(y_test, y_pred),mean_absolute_error(y_test, y_pred)]\n",
    "best_params={'ccp_alpha': 0.00039393774166455076, 'max_depth': 8, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
    "model = DecisionTreeRegressor(random_state=123, **best_params)\n",
    "model.fit(X_train_s, y_train)\n",
    "y_pred = model.predict(X_test_s)\n",
    "result.loc[result.shape[0]] = [\"Decision Tree\", r2_score(y_test, y_pred), mean_squared_error(y_test, y_pred),mean_absolute_error(y_test, y_pred)]\n",
    "best_params =  {'max_depth': 11, 'max_features': 9, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 69}\n",
    "model = RandomForestRegressor(**best_params, verbose=0, n_jobs=-1, random_state=123)\n",
    "model.fit(X_train_s, y_train)\n",
    "y_pred = model.predict(X_test_s)\n",
    "result.loc[result.shape[0]] = [\"Random Forest\", r2_score(y_test, y_pred), mean_squared_error(y_test, y_pred),mean_absolute_error(y_test, y_pred)]\n",
    "\n",
    "best_params = {'colsample_bytree': 0.77, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 2, 'n_estimators': 63, 'subsample': 0.37}\n",
    "other_params = {\"objective\": 'reg:squarederror', \"reg_alpha\": 0, \"reg_lambda\": 1, \"scale_pos_weight\": 1}\n",
    "model = XGBRegressor(**best_params, **other_params, verbose=0, n_jobs=-1, random_state=123)\n",
    "model.fit(X_train_s, y_train)\n",
    "y_pred = model.predict(X_test_s)\n",
    "result.loc[result.shape[0]] = [\"XGBoost\", r2_score(y_test, y_pred), mean_squared_error(y_test, y_pred),mean_absolute_error(y_test, y_pred)]\n",
    "\n",
    "best_params = {\"n_estimators\": 100, \"objective\": 'mse', \"min_split_gain\": 0, 'reg_alpha': 0, 'reg_lambda': 0, \"force_col_wise\": True, 'max_depth': 12, \"learning_rate\": 0.1, 'colsample_bytree': 0.25, \"subsample\": 0.8, 'min_child_samples': 7, \"num_leaves\": 30}\n",
    "model = LGBMRegressor(**best_params, n_jobs=-1, random_state=123, verbosity=-1)\n",
    "model.fit(X_train_s, y_train)\n",
    "y_pred = model.predict(X_test_s)\n",
    "result.loc[result.shape[0]] = [\"LightGBM\", r2_score(y_test, y_pred), mean_squared_error(y_test, y_pred),mean_absolute_error(y_test, y_pred)]\n",
    "\n",
    "params ={\"iterations\": 300, \"learning_rate\": 0.1, \"depth\": 3, \"l2_leaf_reg\": 1, 'bagging_temperature': 0, \"border_count\": 64}\n",
    "model = CatBoostRegressor(**params, random_state=123, train_dir=None, verbose=0)\n",
    "model.fit(X_train_s, y_train, verbose=0\n",
    "          )\n",
    "y_pred = model.predict(X_test_s)\n",
    "result.loc[result.shape[0]] = [\"CatBoost\", r2_score(y_test, y_pred), mean_squared_error(y_test, y_pred),mean_absolute_error(y_test, y_pred)]\n",
    "# result.to_csv('./assert/tables/ml_model_performance.csv', index=False)\n",
    "table_translate(result, filename='机器学习性能评估')\n",
    "result"
   ],
   "id": "dbd11720baec37b2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       model_name  r2_score       mse       mae\n",
       "0             ANN  0.973253  0.003113  0.040779\n",
       "1  OLS Regression       1.0       0.0       0.0\n",
       "2   Decision Tree   0.75058  0.028969  0.128571\n",
       "3   Random Forest  0.882841  0.013607  0.081851\n",
       "4         XGBoost  0.935021  0.007547  0.059592\n",
       "5        LightGBM   0.91969  0.009328  0.065122\n",
       "6        CatBoost  0.956862   0.00501  0.045744"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>r2_score</th>\n",
       "      <th>mse</th>\n",
       "      <th>mae</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ANN</td>\n",
       "      <td>0.973253</td>\n",
       "      <td>0.003113</td>\n",
       "      <td>0.040779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OLS Regression</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.75058</td>\n",
       "      <td>0.028969</td>\n",
       "      <td>0.128571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.882841</td>\n",
       "      <td>0.013607</td>\n",
       "      <td>0.081851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.935021</td>\n",
       "      <td>0.007547</td>\n",
       "      <td>0.059592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.91969</td>\n",
       "      <td>0.009328</td>\n",
       "      <td>0.065122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>0.956862</td>\n",
       "      <td>0.00501</td>\n",
       "      <td>0.045744</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import shap\n",
    "from types import MethodType\n",
    "# import matplotlib.pyplot as plt\n",
    "from sklearn.inspection import permutation_importance\n",
    "# plt.rcParams['font.sans-serif'] = ['SimHei'] # 用来正常显示中文标签\n",
    "# plt.rcParams['axes.unicode_minus'] = False # 用来正常显示负号\n",
    "\n",
    "\n",
    "score = lambda self, X, y: self.evaluate(X, y, verbose=0)[1]\n",
    "# (1)权重特征重要度；\n",
    "model = models.load_model(\"./assert/bp_keras_model.keras\")\n",
    "model.score = MethodType(score, model)\n",
    "layer_weights = model.layers[0].get_weights()[0]\n",
    "feature_importance = np.mean(np.abs(layer_weights), axis=1)  # 取绝对值的平均值\n",
    "\n",
    "# (2)对训练集进行置换重要性分析：计算时间非常长，大概需要4分钟\n",
    "# result = permutation_importance(model, X_train_s, y_train, n_repeats=200, random_state=42, n_jobs=-1)\n",
    "# # n_repeats=20：进行50次随机替换\n",
    "# # Bunch_result:[importance(置换重要度),importance_mean（置换重要度均值）,importance_std（置换重要度标准差）]\n",
    "# result.importances_mean\n",
    "# # Bunch_index = Bunch_result.importances_mean.argsort()\n",
    "# ax.boxplot(Bunch_result.importances[Bunch_index].T, vert=False, labels=nlabels)  #  labels=Xnames\n",
    "\n",
    "# 检验神经网络模型对 2010 年 164 家上市公司经营绩效的拟合效果，计算神经网络绩效评 价得分与经营绩效实际得分的 Pearson 相关系数，其值为 0.966，均方误差为 0.0013，平均绝 对误差为 0.027，平均相对误差为 3.89%，由此可知 BP 神经网络评价模型对 2010 年民营制造 业上市公司经营绩效的拟合效果非常好。从排名差异来看，由表 4.4 可知，排名前五的上市 公司实际排名和预测排名完全一致，排名后五的上市公司两者排名相差较小；检验排名的次 序相关程度，计算 Spearman 次序相关系数，其值为 0.922，说明预测排名与实际排名基本一 致，因此可以认为 BP 神经网络评价模型对 2010 年 164 家民营制造业上市公司经营绩效的拟 合效果非常好。\n",
    "shap.initjs()\n",
    "explainer = shap.DeepExplainer(model, X_train_s)\n",
    "shap_values = explainer.shap_values(X_test_s)\n",
    "_shap_values = shap_values.T[0].T\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(8, 6), dpi=300)\n",
    "\n",
    "# bars = ax.barh(features, feature_importance, left=0, height=0.5, color='skyblue')\n",
    "shap.summary_plot(_shap_values, feature_names=features, sort=False)  # , plot_type=\"bar\", plot_type='bar'\n",
    "\n",
    "# 设置标题\n",
    "# plt.title('各类别数据展示')\n",
    "# ax.set_xlabel('重要度')\n",
    "# ax.set_ylabel('特征')\n",
    "# plt.grid(axis='x', alpha=0.5, linestyle='--')\n",
    "\n",
    "# , transparent=True, bbox_inches='tight'\n",
    "# plt.show()"
   ],
   "id": "eca69a8a27fa5181"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\"\"\"'\n",
    "任务：\n",
    "1.如果计算时间过长，可以先使用 RandomizedSearchCV 进行粗略搜索，再用 GridSearchCV 精调。\n",
    "2.对于 CatBoost 和 LightGBM，可以通过设置 early_stopping_rounds 参数加快训练。\n",
    "3.特征重要性分析：\n",
    "    •XGBoost: model.best_estimator_.feature_importances_\n",
    "    •LightGBM: model.best_estimator_.booster_.feature_importance()\n",
    "    •CatBoost: model.best_estimator_.get_feature_importance()\n",
    "\n",
    "\n",
    "# 检验神经网络模型对 2010 年 164 家上市公司经营绩效的拟合效果，计算神经网络绩效评 价得分与经营绩效实际得分的 Pearson 相关系数，其值为 0.966，均方误差为 0.0013，平均绝 对误差为 0.027，平均相对误差为 3.89%，由此可知 BP 神经网络评价模型对 2010 年民营制造 业上市公司经营绩效的拟合效果非常好。从排名差异来看，由表 4.4 可知，排名前五的上市 公司实际排名和预测排名完全一致，排名后五的上市公司两者排名相差较小；检验排名的次 序相关程度，计算 Spearman 次序相关系数，其值为 0.922，说明预测排名与实际排名基本一 致，因此可以认为 BP 神经网络评价模型对 2010 年 164 家民营制造业上市公司经营绩效的拟 合效果非常好。\n",
    "\"\"\""
   ],
   "id": "346f179d6042eaa8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "adc6fed4b380b597"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
